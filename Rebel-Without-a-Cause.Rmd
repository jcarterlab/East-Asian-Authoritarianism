---
title: "Rebel Without a Cause"
author: "Jack Carter"
date: "13/10/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(dplyr.summarise.inform = FALSE)
library(dplyr, warn.conflicts = FALSE)
library(tidyverse)
library(ggrepel)
library(readxl)
library(ggplot2)
library(ggthemes)
library(here)
library(rtweet)
library(knitr)
library(kableExtra)
library(tidytext)
library(jsonlite)

# read in spreadsheet data without including duplicate 
# tweets  from the previous week's search results.  
read_data <- function(file_name) {
  file <- read_csv(here("data", file_name))
  return(file[,c(1,2,4,5,6,7,9)])
}

# performs the read_data function for all of the data frames
# and joins them into a single tibble data frame. 
read_all_data <- function(dataframes) {
  data <- list()
  for(i in 1:length(dataframes)) {
    data[[i]] <- read_data(file_name = paste0(dataframes[i], ".csv"))
  }
  return(as_tibble(rbind_pages(data)))
}

# breaks the text down into single words, 
# inner joins them with the NRC library 
# and counts the frequency of each sentiment. 
join_words_with_sentiment <- function(raw_tweets) {
  nrc_sentiment <- get_sentiments("nrc") %>% 
    select(word, sentiment)
  words <- raw_tweets %>%
    unnest_tokens(word, text) %>%
    left_join(nrc_sentiment, by = "word") %>%
    mutate(sentiment = replace_na(sentiment, replace = "none")) %>%
    filter(!word %in% stop_words$word) %>%
    group_by(search, region, country, sentiment, week) %>%
    count(sentiment) %>%
    rename(words = "n")
  return(words)
}

# calculates the sentiment for each region
# as a percentage of total words. 
calculate_regional_percentages <- function(words) {
  final_value <- words %>%
    group_by(region) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, sentiment, percent) %>%
    group_by(region, sentiment) %>%
    summarise(percent = sum(percent))
  return(final_value)
}

# calculates the sentiment for each week
# as a percentage of total words. 
calculate_week_percentages <- function(words) {
  final_value <- words %>%
    group_by(region, week) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, sentiment, percent, week) %>%
    group_by(region, sentiment, week) %>%
    summarise(percent = sum(percent))
  return(final_value)
}

# calculates the sentiment for each search term
# as a percentage of total words. 
calculate_search_term_percentages <- function(words) {
  final_value <- words %>%
    group_by(region, search) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, search, sentiment, percent) %>%
    group_by(region, search, sentiment) %>%
    summarise(percent = sum(percent))
  return(final_value)
}

# calculates the sentiment for each country
# as a percentage of total words. 
calculate_country_percentages <- function(words) {
  final_value <- words %>%
    group_by(country, region) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, country, sentiment, percent) %>%
    group_by(region, country, sentiment) %>%
    summarise(percent = sum(percent))
  return(final_value)
}

# a list of different data frame names.  
dataframes <- c("11-05-2021", "18-05-2021", "25-05-2021",
                "01-06-2021", "08-06-2021", "15-06-2021", 
                "22-06-2021", "29-06-2021", "06-07-2021", 
                "13-07-2021", "20-07-2021", "27-07-2021", 
                "03-08-2021", "10-08-2021", "17-08-2021", 
                "24-08-2021", "31-08-2021", "07-09-2021", 
                "14-09-2021", "21-09-2021", "28-09-2021", 
                "05-10-2021", "12-10-2021", "19-10-2021",
                "26-10-2021", "02-11-2021", "09-11-2021",
                "16-11-2021", "23-11-2021", "30-11-2021",
                "07-12-2021", "14-12-2021", "22-12-2021")

# a combined list of raw data. 
raw_data <- read_all_data(dataframes)

# rename regions.   
raw_data[raw_data=="Sinic"] <- "Sinosphere"
raw_data[raw_data=="Anglo"] <- "Anglosphere"

# creates an index of non-duplicated tweets.
index <- which(!duplicated(raw_data[,1]))
data <- raw_data[index,]

# counts the frequency of different NRC sentiments.  
words <- join_words_with_sentiment(data) %>%
  spread(sentiment, words) %>%
  rename("Anticipation" = "anticipation", 
         "Joy" = "joy", 
         "Trust" = "trust",
         "Anger" = "anger", 
         "Disgust" = "disgust", 
         "Fear" = "fear") %>%
  gather(sentiment, words, -c(1:4)) %>%
  mutate(words = replace_na(words, replace = 0)) 

# Calculates each sentiment as a percentage of 
# total words based on region. 
regional_percentages <- calculate_regional_percentages(words)

# Calculates each sentiment as a percentage of 
# total words based on week.  
week_percentages <- calculate_week_percentages(words)

# calculates each sentiment as a percentage of 
# total words based on search term.  
search_term_percentages <- calculate_search_term_percentages(words) 

# calculates each sentiment as a percentage of 
# total words based on country.  
country_percentages <- calculate_country_percentages(words)

# my theme
my_theme <- theme_economist_white(gray_bg = FALSE) +
  theme(plot.title = element_text(hjust = 0.5, 
                                  vjust = 12, 
                                  size = 10, 
                                  color = "#474747"),
        plot.margin = unit(c(0.5, 1, 1, 1), "cm"),
        axis.text = element_text(size = 9, 
                                 color = "gray30"),
        axis.text.x=element_text(vjust = -2.5),
        axis.title.x = element_text(size = 9, 
                                    color = "gray30", 
                                    vjust = -10),
        axis.title.y = element_text(size = 9, 
                                    color = "gray30", 
                                    vjust = 10),
        legend.direction = "vertical", 
        legend.position = "right",
        legend.title = element_blank(),
        legend.text = element_text(size = 12, 
                                   color = "gray20"),
        legend.margin=margin(1, -15, 1, 0),
        legend.spacing.x = unit(0.25, "cm"),
        legend.key.size = unit(1, "cm"), 
        legend.key.height = unit(0.75, "cm"),
        strip.text = element_text(hjust = 0.5, 
                                  vjust = 1, 
                                  size = 10, 
                                  color = "#474747"),
        panel.spacing = unit(2, "lines"))

```

## **Summary**

This project uses sentiment analysis to examine 1.06 million tweets containing, or in response to those containing, one of nine anti-authority words between Anglosphere and Sinosphere Twitter users. Like James Deanâ€™s character in Rebel Without a Cause, Anglosphere users appear to view rebellious acts more positively than others.

&nbsp;

## Results

### **1) Region:**

There is significantly lower net sentiment among Sinosphere users for the anti-authority terms overall.

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# Region
regional_percentages %>%
  spread(sentiment, percent) %>%
  mutate(net = positive - negative) %>%
  gather(sentiment, percent, -1) %>%
  filter(sentiment == "net") %>%
  ggplot(aes(x = region, y = percent, fill = region)) +
  geom_bar(stat = "identity", position = "dodge") +
  ggtitle("") +
  ylab("Net Sentiment (%)") +
  xlab("") +
  scale_fill_manual(values = c("#0072B2", "#D55E00")) +
  my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

```

&nbsp;

### **2) Search Term:**

There is also significantly lower net sentiment among Sinosphere users for each anti-authority term.

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# search terms
search_term_percentages %>%
  spread(sentiment, percent) %>%
  mutate(net = positive - negative) %>%
  gather(sentiment, percent, -c(1:2)) %>%
  filter(sentiment == "net") %>%
  ggplot(aes(x = search, y = percent, fill = region)) +
  geom_bar(stat = "identity", position = "dodge") +
  ggtitle("") +
  ylab("Net Sentiment (%)") +
  xlab("") +
  scale_fill_manual(values = c("#0072B2", "#D55E00")) +
  my_theme +
  scale_x_discrete(guide = guide_axis(n.dodge=2))

```

&nbsp;

### **3) Country:**

There is also lower net sentiment among the majority of Sinosphere countries.

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# selected countries 
country_percentages %>%
  spread(sentiment, percent) %>%
  mutate(net = positive - negative,
         country = factor(country, 
                          levels=c("Australia", "Canada", "New Zealand", 
                                   "South Africa", "UK", "US", 
                                   "China",  "Hong Kong", "Singapore", 
                                   "South Korea", "Taiwan", "Vietnam"))) %>%
  gather(sentiment, percent, -c(1:3)) %>%
  filter(sentiment == "net") %>%
  ggplot(aes(x = country, y = percent, fill = region)) +
  geom_bar(stat = "identity", position = "dodge") +
  ggtitle("") +
  ylab("Net Sentiment (%)") +
  xlab("") +
  scale_fill_manual(values = c("#0072B2", "#D55E00")) +
  my_theme +
  scale_x_discrete(guide = guide_axis(n.dodge=2))

```

&nbsp;

## **Disclaimer**

The data above does not take into account the context of these tweets. Although the high number (over 1 million) of tweets may suggest a statistically significant relationship between higher anti-authority sentiment and Anglosphere users, a more targeted study on individual protest events would help to confirm this.

&nbsp;

## **Method**

### **1) Choose Terms:**

The terms were chosen by searching for synonyms of protest.

**Tweets (000s)**

```{r, echo = FALSE, message = FALSE, warning = FALSE}

kable(spread(data.frame(round(table(data$search)/10^3, 0)), 1, 2))

```

&nbsp;

### **2) Countries:**

The countries were selected because Anglosphere (Western) and Sinosphere (Confucian) cultures were thought to be the most different in terms of attitudes towards authority according to Hoefstedeâ€™s research on power distance.

**Tweets (000s)**

```{r, echo = FALSE, message = FALSE, warning = FALSE}

kable(spread(data.frame(round(table(data$country)/10^3, 0)), 1, 2))

```

&nbsp;

### **3) Data Collection:**

The tweets were collected from a 50 mile range of the each country capital every week from 11 May to 22 December 2021.

---EXAMPLE CODE SNIPET---

```{r, echo = TRUE, message = FALSE, warning = FALSE}

# searches for tweets with a given search term in a given 
# location using the rtweet search_tweets function. 
get_tweets <- function(search_term, coordinates) {
  search_tweets(
    q = search_term,
    n = 100000,
    include_rts = FALSE,
    retryonratelimit = TRUE,
    lang = "en",
    geocode = coordinates
  )
}

```

&nbsp;

### **4) Data Cleaning:**

The text is cleaned by removing links and converting all characters to lowercase.  

---EXAMPLE CODE SNIPET---

```{r, echo = TRUE, message = FALSE, warning = FALSE}

# cleans links from the text. 
process_raw_tweet_data <- function(country) {
  pattern <- "https://t.co/[A-Za-z\\d]+|&amp;"
  text <- country %>%
    mutate(text = str_to_lower(str_replace_all(text, pattern, "")))
  return(text)
}

```

&nbsp;
 
### **5) Sentiment Analysis:**

A sentiment analysis is calculated by breaking down each tweet into individual words, removing stopwords (common words with little sentiment value) and calculating the percentage of positive minus negative words.

---EXAMPLE CODE SNIPET---

```{r, echo = TRUE, message = FALSE, warning = FALSE}

# calculates sentiment values faceted by region, search term and sentiment type. 
calculate_search_term_percentages <- function(words) {
  final_value <- words %>%
    group_by(region, search) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, search, sentiment, percent) %>%
    group_by(region, search, sentiment) %>%
    summarise(percent = sum(percent))
  return(final_value)
}

```

&nbsp;

## **Sources**

- Clearly Cultural (2022) https://clearlycultural.com/geert-hofstede-cultural-dimensions/power-distance-index/

- Mohammad (2021) https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm

- Twitter (2021) https://developer.twitter.com/en/apply-for-access

&nbsp;
&nbsp;
&nbsp;
