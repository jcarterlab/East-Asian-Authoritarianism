---
title: "Rebel Without a Cause"
author: "Jack Carter"
date: "13/10/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(dplyr.summarise.inform = FALSE)
library(dplyr, warn.conflicts = FALSE)
library(tidyverse)
library(ggrepel)
library(readxl)
library(ggplot2)
library(ggthemes)
library(here)
library(rtweet)
library(knitr)
library(kableExtra)
library(tidytext)
library(jsonlite)

# read in spreadsheet data without including duplicate 
# tweets  from the previous week's search results.  
read_data <- function(file_name) {
  file <- read_csv(
    here("data", file_name)
  )
  return(file[,c(1,2,4,5,6,7,9)])
}

# performs the read_data function for all of the data frames
# and joins them into a single tibble data frame. 
read_all_data <- function(dataframes) {
  data <- list()
  for(i in 1:length(dataframes)) {
    data[[i]] <- read_data(
      file_name = paste0(dataframes[i], ".csv")
    )
  }
  return(as_tibble(rbind_pages(data)))
}

# breaks the text down into single words, 
# inner joins them with the NRC library 
# and counts the frequency of each sentiment. 
join_words_with_sentiment <- function(raw_tweets) {
  nrc_sentiment <- get_sentiments("nrc") %>% 
    select(
      word, sentiment
    )
  words <- raw_tweets %>%
    unnest_tokens(
      word, text
    ) %>%
    left_join(
      nrc_sentiment, by = "word"
    ) %>%
    mutate(
      sentiment = replace_na(sentiment, replace = "none")
    ) %>%
    filter(
      !word %in% stop_words$word
    ) %>%
    group_by(
      search, region, country, sentiment, week
    ) %>%
    count(
      sentiment
    ) %>%
    rename(
      words = "n"
    )
  return(words)
}

# calculates the sentiment for each region
# as a percentage of total words. 
calculate_regional_percentages <- function(words) {
  final_value <- words %>%
    group_by(
      region
    ) %>%
    mutate(
      total_words = sum(words),
      percent = (words / total_words)*100
    ) %>%
    select(
      region, sentiment, percent
    ) %>%
    group_by(
      region, sentiment
    ) %>%
    summarise(percent = sum(percent)
    )
  return(final_value)
}

# calculates the sentiment for each week
# as a percentage of total words. 
calculate_week_percentages <- function(words) {
  final_value <- words %>%
    group_by(
      region, week
    ) %>%
    mutate(
      total_words = sum(words),
      percent = (words / total_words)*100
    ) %>%
    select(
      region, sentiment, percent, week
    ) %>%
    group_by(
      region, sentiment, week
    ) %>%
    summarise(
      percent = sum(percent)
    )
  return(final_value)
}

# calculates the sentiment for each search term
# as a percentage of total words. 
calculate_search_term_percentages <- function(words) {
  final_value <- words %>%
    group_by(
      region, search
    ) %>%
    mutate(
      total_words = sum(words),
      percent = (words / total_words)*100
    ) %>%
    select(
      region, search, sentiment, percent
    ) %>%
    group_by(
      region, search, sentiment
    ) %>%
    summarise(
      percent = sum(percent)
    )
  return(final_value)
}

# calculates the sentiment for each country
# as a percentage of total words. 
calculate_country_percentages <- function(words) {
  final_value <- words %>%
    group_by(
      country, region
    ) %>%
    mutate(
      total_words = sum(words),
      percent = (words / total_words)*100
    ) %>%
    select(
      region, country, sentiment, percent
    ) %>%
    group_by(
      region, country, sentiment
    ) %>%
    summarise(
      percent = sum(percent)
    )
  return(final_value)
}

# a list of different data frame names.  
dataframes <- c("11-05-2021", "18-05-2021", "25-05-2021",
                "01-06-2021", "08-06-2021", "15-06-2021", 
                "22-06-2021", "29-06-2021", "06-07-2021", 
                "13-07-2021", "20-07-2021", "27-07-2021", 
                "03-08-2021", "10-08-2021", "17-08-2021", 
                "24-08-2021", "31-08-2021", "07-09-2021", 
                "14-09-2021", "21-09-2021", "28-09-2021", 
                "05-10-2021", "12-10-2021", "19-10-2021",
                "26-10-2021", "02-11-2021", "09-11-2021",
                "16-11-2021", "23-11-2021", "30-11-2021",
                "07-12-2021", "14-12-2021", "22-12-2021")

# a combined list of raw data. 
raw_data <- read_all_data(
  dataframes
)

# rename regions.   
raw_data[raw_data=="Sinic"] <- "Sino"
raw_data[raw_data=="Anglo"] <- "Anglo"

# creates an index of non-duplicated tweets.
index <- which(
  !duplicated(raw_data[,1])
)
data <- raw_data[index,]

# counts the frequency of different NRC sentiments.  
words <- join_words_with_sentiment(
  data
) %>%
  spread(
    sentiment, words
  ) %>%
  rename(
    "Anticipation" = "anticipation", 
    "Joy" = "joy", 
    "Trust" = "trust",
    "Anger" = "anger", 
    "Disgust" = "disgust", 
    "Fear" = "fear"
  ) %>%
  gather(
    sentiment, words, -c(1:4)
  ) %>%
  mutate(
    words = replace_na(words, replace = 0)
  ) 

# Calculates each sentiment as a percentage of 
# total words based on region. 
regional_percentages <- calculate_regional_percentages(words)

# Calculates each sentiment as a percentage of 
# total words based on week.  
week_percentages <- calculate_week_percentages(words)

# calculates each sentiment as a percentage of 
# total words based on search term.  
search_term_percentages <- calculate_search_term_percentages(words) 

# calculates each sentiment as a percentage of 
# total words based on country.  
country_percentages <- calculate_country_percentages(words)

# my theme
my_theme <- theme_economist_white(gray_bg = FALSE) +
  theme(plot.title = element_text(hjust = 0.5, 
                                  vjust = 12, 
                                  size = 10, 
                                  color = "#474747"),
        plot.margin = unit(c(0.5, 1, 1, 1), "cm"),
        axis.text = element_text(size = 9, 
                                 color = "gray30"),
        axis.text.x=element_text(vjust = -2.5),
        axis.title.x = element_text(size = 9, 
                                    color = "gray30", 
                                    vjust = -10),
        axis.title.y = element_text(size = 9, 
                                    color = "gray30", 
                                    vjust = 10),
        legend.direction = "vertical", 
        legend.position = "right",
        legend.title = element_blank(),
        legend.text = element_text(size = 12, 
                                   color = "gray20"),
        legend.margin=margin(1, -15, 1, 0),
        legend.spacing.x = unit(0.25, "cm"),
        legend.key.size = unit(1, "cm"), 
        legend.key.height = unit(0.75, "cm"),
        strip.text = element_text(hjust = 0.5, 
                                  vjust = 1, 
                                  size = 10, 
                                  color = "#474747"),
        panel.spacing = unit(2, "lines"))

```

## **Summary**

This project analyzes a corpus of `r round(length(index)/10^6, 2)` million tweets containing nine anti-authority words between Sinosphere and Anglosphere Twitter users. It finds that negative sentiment is significantly higher among Sinosphere users, potentially indicating a deep rooted cultural difference in attitudes towards authority.   

&nbsp;

## Results

### **1) Region:**

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# Region
regional_percentages %>%
  spread(
    sentiment, percent
    ) %>%
  mutate(
    net = positive - negative
    ) %>%
  gather(
    sentiment, percent, -1
    ) %>%
  filter(
    sentiment == "net"
    ) %>%
  ggplot(
    aes(x = region, y = percent, fill = region)
    ) +
  geom_bar(
    stat = "identity", position = "dodge"
    ) +
  ggtitle(
    ""
    ) +
  ylab(
    "Net Sentiment (%)"
    ) +
  xlab("") +
  scale_fill_manual(
    values = c("#0072B2", "#D55E00")
    ) +
  my_theme

```

<br/>

### **2) Search Term:**

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# search terms
search_term_percentages %>%
  spread(
    sentiment, percent
    ) %>%
  mutate(
    net = positive - negative
    ) %>%
  gather(
    sentiment, percent, -c(1:2)
    ) %>%
  filter(
    sentiment == "net"
    ) %>%
  ggplot(
    aes(x = search, y = percent, fill = region)
    ) +
  geom_bar(
    stat = "identity", position = "dodge"
    ) +
  ggtitle(
    ""
    ) +
  ylab(
    "Net Sentiment (%)"
    ) +
  xlab("") +
  scale_fill_manual(
    values = c("#0072B2", "#D55E00")
    ) +
  my_theme +
  scale_x_discrete(
    guide = guide_axis(n.dodge=2)
    )

```

<br/>

### **3) Country:**

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# selected countries 
country_percentages %>%
  spread(
    sentiment, percent
    ) %>%
  mutate(
    net = positive - negative,
         country = factor(country, 
                          levels=c("Australia", "Canada", "New Zealand", 
                                   "South Africa", "UK", "US", 
                                   "China",  "Hong Kong", "Singapore", 
                                   "South Korea", "Taiwan", "Vietnam"))
    ) %>%
  gather(
    sentiment, percent, -c(1:3)
    ) %>%
  filter(
    sentiment == "net"
    ) %>%
  ggplot(
    aes(x = country, y = percent, fill = region)
    ) +
  geom_bar(
    stat = "identity", position = "dodge"
    ) +
  ggtitle(
    ""
    ) +
  ylab(
    "Net Sentiment (%)"
    ) +
  xlab("") +
  scale_fill_manual(
    values = c("#0072B2", "#D55E00")
    ) +
  my_theme +
  scale_x_discrete(
    guide = guide_axis(n.dodge=2)
    )

```

&nbsp;

## **Method**

### **1) Search Terms:**

The nine anti-authority search terms and their tweet frequencies in thousands:

```{r, echo = FALSE, message = FALSE, warning = FALSE}

kable(spread(data.frame(round(table(data$search)/10^3, 0)), 1, 2))

```

&nbsp;

### **2) Countries:**

The twelve countries and their tweet frequencies in thousands:

```{r, echo = FALSE, message = FALSE, warning = FALSE}

kable(spread(data.frame(round(table(data$country)/10^3, 0)), 1, 2))

```

&nbsp;

### **3) Data Collection:**

The tweets were collected using Twitter location data from within a 50 mile range of the respective country capitals every week from May 11 to December 22, 2021.

---EXAMPLE CODE SNIPET---

```{r, echo = TRUE, message = FALSE, warning = FALSE}

# collects tweets for each search term and binds the 
# results together into a tibble data frame. 
process_country <- function(country, region, coordinates) {
  searches <- list() 
  for(i in 1:length(search_term)){
    searches[[i+1]] <- process_data(search_term[i], coordinates, country, 
                                    region, date_collected, label[i])
  }
  result <- tibble(rbind_pages(searches))
  return(result)
} 


```


&nbsp;

### **4) Data Cleaning:**

The text is cleaned to remove links.  

---EXAMPLE CODE SNIPET---

```{r, echo = TRUE, message = FALSE, warning = FALSE}

# cleans links from the text. 
process_raw_tweet_data <- function(country) {
  pattern <- "https://t.co/[A-Za-z\\d]+|&amp;"
  text <- country %>%
    mutate(text = str_to_lower(str_replace_all(text, pattern, "")))
  return(text)
}

```

&nbsp;
 
### **5) Sentiment Analysis:**

A sentiment analysis is conducted by breaking down each tweet into individual words (tokens), removing stopwords (common words with little sentiment value) and calculating the net percentage of positive verses negative words. 

---EXAMPLE CODE SNIPET---

```{r, echo = TRUE, message = FALSE, warning = FALSE}

# calculates sentiment values faceted by region, search term and sentiment type. 
calculate_search_term_percentages <- function(words) {
  final_value <- words %>%
    group_by(region, search) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, search, sentiment, percent) %>%
    group_by(region, search, sentiment) %>%
    summarise(percent = sum(percent))
  return(final_value)
}

```

&nbsp;

## **Sources**

- Mohammad (2021) https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm

- Twitter (2021) https://developer.twitter.com/en/apply-for-access

&nbsp;
&nbsp;
&nbsp;
