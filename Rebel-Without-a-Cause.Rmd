---
title: "Rebel Without a Cause"
author: "Jack Carter"
date: "13/10/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(dplyr.summarise.inform = FALSE)
library(dplyr, warn.conflicts = FALSE)
library(tidyverse)
library(ggrepel)
library(readxl)
library(ggplot2)
library(ggthemes)
library(here)
library(rtweet)
library(knitr)
library(kableExtra)
library(tidytext)
library(jsonlite)

# read in spreadsheet data without including duplicate 
# tweets  from the previous week's search results.  
read_data <- function(file_name) {
  file <- read_csv(
    here("data", file_name)
  )
  return(file[,c(1,2,4,5,6,7,9)])
}

# performs the read_data function for all of the data frames
# and joins them into a single tibble data frame. 
read_all_data <- function(dataframes) {
  data <- list()
  for(i in 1:length(dataframes)) {
    data[[i]] <- read_data(
      file_name = paste0(dataframes[i], ".csv")
    )
  }
  return(as_tibble(rbind_pages(data)))
}

# breaks the text down into single words, 
# inner joins them with the NRC library 
# and counts the frequency of each sentiment. 
join_words_with_sentiment <- function(raw_tweets) {
  nrc_sentiment <- get_sentiments("nrc") %>% 
    select(
      word, sentiment
    )
  words <- raw_tweets %>%
    unnest_tokens(
      word, text
    ) %>%
    left_join(
      nrc_sentiment, by = "word"
    ) %>%
    mutate(
      sentiment = replace_na(sentiment, replace = "none")
    ) %>%
    filter(
      !word %in% stop_words$word
    ) %>%
    group_by(
      search, region, country, sentiment, week
    ) %>%
    count(
      sentiment
    ) %>%
    rename(
      words = "n"
    )
  return(words)
}

# calculates the sentiment for each region
# as a percentage of total words. 
calculate_regional_percentages <- function(words) {
  final_value <- words %>%
    group_by(
      region
    ) %>%
    mutate(
      total_words = sum(words),
      percent = (words / total_words)*100
    ) %>%
    select(
      region, sentiment, percent
    ) %>%
    group_by(
      region, sentiment
    ) %>%
    summarise(percent = sum(percent)
    )
  return(final_value)
}

# calculates the sentiment for each week
# as a percentage of total words. 
calculate_week_percentages <- function(words) {
  final_value <- words %>%
    group_by(
      region, week
    ) %>%
    mutate(
      total_words = sum(words),
      percent = (words / total_words)*100
    ) %>%
    select(
      region, sentiment, percent, week
    ) %>%
    group_by(
      region, sentiment, week
    ) %>%
    summarise(
      percent = sum(percent)
    )
  return(final_value)
}

# calculates the sentiment for each search term
# as a percentage of total words. 
calculate_search_term_percentages <- function(words) {
  final_value <- words %>%
    group_by(
      region, search
    ) %>%
    mutate(
      total_words = sum(words),
      percent = (words / total_words)*100
    ) %>%
    select(
      region, search, sentiment, percent
    ) %>%
    group_by(
      region, search, sentiment
    ) %>%
    summarise(
      percent = sum(percent)
    )
  return(final_value)
}

# calculates the sentiment for each country
# as a percentage of total words. 
calculate_country_percentages <- function(words) {
  final_value <- words %>%
    group_by(
      country, region
    ) %>%
    mutate(
      total_words = sum(words),
      percent = (words / total_words)*100
    ) %>%
    select(
      region, country, sentiment, percent
    ) %>%
    group_by(
      region, country, sentiment
    ) %>%
    summarise(
      percent = sum(percent)
    )
  return(final_value)
}

# a list of different data frame names.  
dataframes <- c("11-05-2021", "18-05-2021", "25-05-2021",
                "01-06-2021", "08-06-2021", "15-06-2021", 
                "22-06-2021", "29-06-2021", "06-07-2021", 
                "13-07-2021", "20-07-2021", "27-07-2021", 
                "03-08-2021", "10-08-2021", "17-08-2021", 
                "24-08-2021", "31-08-2021", "07-09-2021", 
                "14-09-2021", "21-09-2021", "28-09-2021", 
                "05-10-2021", "12-10-2021", "19-10-2021",
                "26-10-2021")

# a combined list of raw data. 
raw_data <- read_all_data(
  dataframes
)

# rename regions.   
raw_data[raw_data=="Sinic"] <- "Sinosphere"
raw_data[raw_data=="Anglo"] <- "Anglosphere"

# creates an index of non-duplicated tweets.
index <- which(
  !duplicated(raw_data[,1])
)

# counts the frequency of different NRC sentiments.  
words <- join_words_with_sentiment(
  raw_data[index,]
) %>%
  spread(
    sentiment, words
  ) %>%
  rename(
    "Anticipation" = "anticipation", 
    "Joy" = "joy", 
    "Trust" = "trust",
    "Anger" = "anger", 
    "Disgust" = "disgust", 
    "Fear" = "fear"
  ) %>%
  gather(
    sentiment, words, -c(1:4)
  ) %>%
  mutate(
    words = replace_na(words, replace = 0)
  ) 

# Calculates each sentiment as a percentage of 
# total words based on region. 
regional_percentages <- calculate_regional_percentages(words)

# Calculates each sentiment as a percentage of 
# total words based on week.  
week_percentages <- calculate_week_percentages(words)

# calculates each sentiment as a percentage of 
# total words based on search term.  
search_term_percentages <- calculate_search_term_percentages(words) 

# calculates each sentiment as a percentage of 
# total words based on country.  
country_percentages <- calculate_country_percentages(words)

# my theme
my_theme <- theme_economist_white(gray_bg = FALSE) +
  theme(plot.title = element_text(hjust = 0.5, 
                                  vjust = 12, 
                                  size = 10, 
                                  color = "#474747"),
        plot.margin = unit(c(1, 1, 1, 1), "cm"),
        axis.text = element_text(size = 9, 
                                 color = "gray30"),
        axis.text.x=element_text(vjust = -2.5),
        axis.title.x = element_text(size = 9, 
                                    color = "gray30", 
                                    vjust = -10),
        axis.title.y = element_text(size = 9, 
                                    color = "gray30", 
                                    vjust = 10),
        legend.direction = "vertical", 
        legend.position = "right",
        legend.title = element_blank(),
        legend.text = element_text(size = 12, 
                                   color = "gray20"),
        legend.margin=margin(1, -15, 1, 0),
        legend.spacing.x = unit(0.25, "cm"),
        legend.key.size = unit(1, "cm"), 
        legend.key.height = unit(0.75, "cm"),
        strip.text = element_text(hjust = 0.5, 
                                  vjust = 1, 
                                  size = 10, 
                                  color = "#474747"),
        panel.spacing = unit(2, "lines"))

```

## **Summary**

This project analyzes a corpus of over 900,000 tweets containing, or in
response to those containing, a range of anti-authority
words. It aims to test whether there are systematic differences in
anti-authority sentiment between Anglosphere and Sinosphere Twitter
users. It finds that negative sentiment is significantly higher among
Sinosphere users, potentially indicating a deep rooted cultural
difference in attitudes to authority. These findings could also
be a result of other factors though, including a negative reaction Myanmar's 2021 military coup and China's growing regional assertiveness.  

&nbsp;

## **Method**

**1) Search Parameters:**

The anti-authority terms searched for include defy, disobey, dissent, oppose, protest, rebel, resist, revolt and riot, while the countries targeted include Australia, Canada, New Zealand, South Africa, the UK, the US, China, Hong Kong, Singapore, South Korea, Taiwan and Vietnam. 

<br/>

**2) Data Collection:**

The tweets are collected using Twitter location data from within a 50 mile range of the respective country capitals every week from the 11th of May to present. They are then cleaned to exclude duplicates, eliminate stopwords (common words with little sentiment value) and remove punctuation.

<br/>
 
**3) Net Sentiment:**

Net sentiment is calculated as the total share of positive minus negative words. It is expressed as a percentage of all words according to the relevant faceting variables (region, country etc.). This takes into account differences in the number of tweets. 

&nbsp;

## Results

### **1) Region:**

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# Region
regional_percentages %>%
  spread(
    sentiment, percent
    ) %>%
  mutate(
    net = positive - negative
    ) %>%
  gather(
    sentiment, percent, -1
    ) %>%
  filter(
    sentiment == "net"
    ) %>%
  ggplot(
    aes(x = region, y = percent, fill = region)
    ) +
  geom_bar(
    stat = "identity", position = "dodge"
    ) +
  ggtitle(
    ""
    ) +
  ylab(
    "Net Sentiment (%)"
    ) +
  xlab("") +
  scale_fill_manual(
    values = c("#0072B2", "#D55E00")
    ) +
  my_theme

```

**Region Tweets (000s)**

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

knitr::kable(t(round(table(raw_data[index,]$region) / 10^3, 0)))

```

<br/>

### **2) Country:**

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# selected countries 
country_percentages %>%
  spread(
    sentiment, percent
    ) %>%
  mutate(
    net = positive - negative,
         country = factor(country, 
                          levels=c("Australia", "Canada", "New Zealand", 
                                   "South Africa", "UK", "US", 
                                   "China",  "Hong Kong", "Singapore", 
                                   "South Korea", "Taiwan", "Vietnam"))
    ) %>%
  gather(
    sentiment, percent, -c(1:3)
    ) %>%
  filter(
    sentiment == "net"
    ) %>%
  ggplot(
    aes(x = country, y = percent, fill = region)
    ) +
  geom_bar(
    stat = "identity", position = "dodge"
    ) +
  ggtitle(
    ""
    ) +
  ylab(
    "Net Sentiment (%)"
    ) +
  xlab("") +
  scale_fill_manual(
    values = c("#0072B2", "#D55E00")
    ) +
  my_theme +
  scale_x_discrete(
    guide = guide_axis(n.dodge=2)
    )

```

**Country Tweets (000s)**

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

knitr::kable(t(round(table(raw_data[index,]$country) / 10^3, 0)))

```

<br/>

### **3) Search Term:**

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# search terms
search_term_percentages %>%
  spread(
    sentiment, percent
    ) %>%
  mutate(
    net = positive - negative
    ) %>%
  gather(
    sentiment, percent, -c(1:2)
    ) %>%
  filter(
    sentiment == "net"
    ) %>%
  ggplot(
    aes(x = search, y = percent, fill = region)
    ) +
  geom_bar(
    stat = "identity", position = "dodge"
    ) +
  ggtitle(
    ""
    ) +
  ylab(
    "Net Sentiment (%)"
    ) +
  xlab("") +
  scale_fill_manual(
    values = c("#0072B2", "#D55E00")
    ) +
  my_theme +
  scale_x_discrete(
    guide = guide_axis(n.dodge=2)
    )

```

**Search Term Tweets (000s)**

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

knitr::kable(t(round(table(raw_data[index,]$search) / 10^3, 0)))

```

<br/>

### **4) Week Collected:**

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# week
week_percentages %>%
  spread(
    sentiment, percent
    ) %>%
  mutate(
    net = positive - negative
    ) %>%
  gather(
    sentiment, percent, -c(1:2)
    ) %>%
  filter(
    sentiment == "net"
    ) %>%
  ggplot(
    aes(x = week, y = percent, fill = region)
    ) +
  geom_bar(
    stat = "identity", position = "dodge"
    ) +
  ggtitle(
    ""
    ) +
  ylab(
    "Net Sentiment (%)"
    ) +
  xlab("") +
  scale_fill_manual(
    values = c("#0072B2", "#D55E00")
    ) +
  my_theme

```

**Week Collected Tweets (000s)**

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

knitr::kable(t(round(table(raw_data[index,]$week) / 10^3, 0)))

```

<br/>

&nbsp;

## **Sources**

- Mohammad (2021) https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm

- Twitter (2021) https://developer.twitter.com/en/apply-for-access

&nbsp;
&nbsp;
&nbsp;
