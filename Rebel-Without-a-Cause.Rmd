---
title: "Rebel Without a Cause"
author: "Jack Carter"
date: "13/10/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(dplyr.summarise.inform = FALSE)
library(dplyr, warn.conflicts = FALSE)
library(tidyverse)
library(ggrepel)
library(readxl)
library(ggplot2)
library(ggthemes)
library(here)
library(rtweet)
library(knitr)
library(tidytext)
library(jsonlite)

# read in spreadsheet data without including duplicate 
# tweets  from the previous week's search results.  
read_data <- function(file_name) {
  file <- read_csv(here("data", file_name))
  return(file[,c(1,2,4,5,6,7,9)])
}

# performs the read_data function for all of the data frames
# and joins them into a single tibble data frame. 
read_all_data <- function(dataframes) {
  data <- list()
  for(i in 1:length(dataframes)) {
    data[[i]] <- read_data(file_name = paste0(dataframes[i], 
                                              ".csv"))
  }
  return(as_tibble(rbind_pages(data)))
}

# breaks the text down into single words, 
# inner joins them with the NRC library 
# and counts the frequency of each sentiment. 
join_words_with_sentiment <- function(raw_tweets) {
  nrc_sentiment <- get_sentiments("nrc") %>% select(word, sentiment)
  words <- raw_tweets %>%
    unnest_tokens(word, text) %>%
    left_join(nrc_sentiment, by = "word") %>%
    mutate(sentiment = replace_na(sentiment, replace = "none")) %>%
    filter(!word %in% stop_words$word) %>%
    group_by(search, region, country, sentiment, week) %>%
    count(sentiment) %>%
    rename(words = "n")
    return(words)
}

# calculates the sentiment for each region
# as a percentage of total words. 
calculate_regional_percentages <- function(words) {
  final_value <- words %>%
    group_by(region) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, sentiment, percent) %>%
    group_by(region, sentiment) %>%
    summarise(percent = sum(percent))
}

# calculates the sentiment for each week
# as a percentage of total words. 
calculate_week_percentages <- function(words) {
  final_value <- words %>%
    group_by(region, week) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, sentiment, percent, week) %>%
    group_by(region, sentiment, week) %>%
    summarise(percent = sum(percent))
}

# calculates the sentiment for each search term
# as a percentage of total words. 
calculate_search_term_percentages <- function(words) {
  final_value <- words %>%
    group_by(region, search) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, search, sentiment, percent) %>%
    group_by(region, search, sentiment) %>%
    summarise(percent = sum(percent))
}

# calculates the sentiment for each country
# as a percentage of total words. 
calculate_country_percentages <- function(words) {
  final_value <- words %>%
    group_by(country, region) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, country, sentiment, percent) %>%
    group_by(region, country, sentiment) %>%
    summarise(percent = sum(percent))
}

# a list of different data frame names.  
dataframes <- c("11-05-2021", "18-05-2021", "25-05-2021",
                "01-06-2021", "08-06-2021", "15-06-2021", 
                "22-06-2021", "29-06-2021", "06-07-2021", 
                "13-07-2021", "20-07-2021", "27-07-2021", 
                "03-08-2021", "10-08-2021", "17-08-2021", 
                "24-08-2021", "31-08-2021", "07-09-2021", 
                "14-09-2021", "21-09-2021", "28-09-2021", 
                "05-10-2021", "12-10-2021")

# a combined list of raw data. 
raw_data <- read_all_data(dataframes)

# replace Sinic region name with Sino.  
raw_data[raw_data=="Sinic"] <- "Sino"

# creates an index of non-duplicated tweets.
index <- which(!duplicated(raw_data[,1]))

# counts the frequency of different NRC sentiments.  
words <- join_words_with_sentiment(raw_data[index,]) %>%
  spread(sentiment, words) %>%
  rename("Anticipation" = "anticipation", 
         "Joy" = "joy", 
         "Trust" = "trust",
         "Anger" = "anger", 
         "Disgust" = "disgust", 
         "Fear" = "fear") %>%
  gather(sentiment, words, -c(1:4)) %>%
  mutate(words = replace_na(words, replace = 0)) 

# Calculates each sentiment as a percentage of 
# total words based on region. 
regional_percentages <- calculate_regional_percentages(words)

# Calculates each sentiment as a percentage of 
# total words based on week.  
week_percentages <- calculate_week_percentages(words)

# calculates each sentiment as a percentage of 
# total words based on search term.  
search_term_percentages <- calculate_search_term_percentages(words) 

# calculates each sentiment as a percentage of 
# total words based on country.  
country_percentages <- calculate_country_percentages(words) 

# my personal plot theme for data visualizations. 
my_theme <- theme_economist_white(gray_bg = FALSE) +
  theme(plot.title = element_text(hjust = 0.5, 
                                  vjust = 12, 
                                  size = 9, 
                                  color = "#474747"),
        plot.margin = unit(c(1.5, 1, 1.5, 1), "cm"),
        axis.text = element_text(size = 9, 
                                 color = "gray30"),
        axis.text.x=element_text(vjust = -2.5),
        axis.title.x = element_text(size = 9, 
                                    color = "gray30", 
                                    vjust = -10),
        axis.title.y = element_text(size = 9, 
                                    color = "gray30", 
                                    vjust = 10),
        legend.direction = "vertical", 
        legend.position = "right",
        legend.title = element_blank(),
        legend.text = element_text(size = 12, 
                                   color = "gray20"),
        legend.margin=margin(1, -15, 1, 0),
        legend.spacing.x = unit(0.25, "cm"),
        legend.key.size = unit(1, "cm"), 
        legend.key.height = unit(0.75, "cm"),
        strip.text = element_text(hjust = 0.5, 
                                  vjust = 1, 
                                  size = 10, 
                                  color = "#474747"),
        panel.spacing = unit(2, "lines"))

```

## **Summary**

This project analyzes a corpus of over 900,000 tweets containing, or in response to tweets containing, a range of different anti-authority words. It aims to test whether there are systematic differences in anti-authority sentiment between Anglosphere and Sinosphere Twitter users. It finds that negative sentiment is significantly higher among Sinosphere users, potentially indicating a deep rooted cultural difference in attitudes to authority. These results could also however be due to other factors, such as the more limited use of anti-authority words in everyday language for those whose other tongue is not English.

&nbsp;

## **Method**

**1) Search Terms:**

The tweets contain or are in response to those containing one or more of nine anti-authority words, including defy, disobey, dissent, oppose, protest, rebel, resist, revolt and riot.

**2) Countries:**

The Anglosphere countries include Australia, Canada, New Zealand, South Africa, the UK and the US. The Sinosphere countries include China, Hong Kong, Singapore, South Korea, Taiwan and Vietnam. 

**3) Data Collection:**

The tweets are collected using Twitter location data from within a 50 mile range of the country capitals each week from the 11th of May to present. They are filtered to exclude duplicates before being cleaned to eliminate stop words (those adding no sentiment value) and punctuation.
 
**4) Sentiment Calculation:**

Net sentiment is then calculated as the total share of positive words minus the total share of negative words. It is expressed in each visualization as a percentage of all words, taking into account differences in the number of and length of tweets.

&nbsp;

### Results:

1. Net sentiment by region. 

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# Region
regional_percentages %>%
  spread(sentiment, percent) %>%
  mutate(net = positive - negative) %>%
  gather(sentiment, percent, -1) %>%
  filter(sentiment == "net") %>%
  ggplot(aes(x = region, y = percent, fill = region)) +
  geom_bar(stat = "identity", position = "dodge") +
  ggtitle("Region") +
  ylab("Net Sentiment") +
  xlab("") +
  scale_fill_manual(values = c("#0072B2", "#D55E00")) +
  my_theme

```

2. Net sentiment by week.  

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# week
week_percentages %>%
  spread(sentiment, percent) %>%
  mutate(net = positive - negative) %>%
  gather(sentiment, percent, -c(1:2)) %>%
  filter(sentiment == "net") %>%
  ggplot(aes(x = week, y = percent, fill = region)) +
  geom_bar(stat = "identity", position = "dodge") +
  ggtitle("Week") +
  ylab("Net Sentiment") +
  xlab("") +
  scale_fill_manual(values = c("#0072B2", "#D55E00")) +
  my_theme

```

3. Net sentiment by search term.    

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# search terms
search_term_percentages %>%
  spread(sentiment, percent) %>%
  mutate(net = positive - negative) %>%
  gather(sentiment, percent, -c(1:2)) %>%
  filter(sentiment == "net") %>%
  ggplot(aes(x = search, y = percent, fill = region)) +
  geom_bar(stat = "identity", position = "dodge") +
  ggtitle("Search Term") +
  ylab("% of Total Words") +
  xlab("") +
  scale_fill_manual(values = c("#0072B2", "#D55E00")) +
  my_theme

```

4. Net sentiment by country.

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# selected countries 
country_percentages %>%
  spread(sentiment, percent) %>%
  mutate(net = positive - negative,
         country = factor(country, 
                          levels=c("Australia", "Canada", "New Zealand", 
                                   "South Africa", "UK", "US", 
                                   "China",  "Hong Kong", "Singapore", 
                                   "South Korea", "Taiwan", "Vietnam"))) %>%
  gather(sentiment, percent, -c(1:3)) %>%
  filter(sentiment == "net") %>%
  ggplot(aes(x = country, y = percent, fill = region)) +
  geom_bar(stat = "identity", position = "dodge") +
  ggtitle("Country") +
  ylab("Net Sentiment") +
  xlab("") +
  ylim(-6.5, 0) +
  scale_fill_manual(values = c("#0072B2", "#D55E00")) +
  my_theme +
  scale_x_discrete(guide = guide_axis(n.dodge=2))


```

&nbsp;

## **Sources**

- Mohammad (2021) https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm

- Twitter (2021) https://developer.twitter.com/en/apply-for-access

&nbsp;
&nbsp;
&nbsp;
