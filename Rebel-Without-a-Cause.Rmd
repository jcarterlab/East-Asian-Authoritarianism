---
title: "Rebel Without a Cause"
author: "Jack Carter"
date: "13/10/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(dplyr.summarise.inform = FALSE)
library(dplyr, warn.conflicts = FALSE)
library(tidyverse)
library(ggrepel)
library(readxl)
library(ggplot2)
library(ggthemes)
library(here)
library(rtweet)
library(knitr)
library(kableExtra)
library(tidytext)
library(jsonlite)

# read in spreadsheet data without including duplicate 
# tweets  from the previous week's search results.  
read_data <- function(file_name) {
  file <- read_csv(here("data", file_name))
  return(file[,c(1,2,4,5,6,7,9)])
}

# performs the read_data function for all of the data frames
# and joins them into a single tibble data frame. 
read_all_data <- function(dataframes) {
  data <- list()
  for(i in 1:length(dataframes)) {
    data[[i]] <- read_data(file_name = paste0(dataframes[i], ".csv"))
  }
  return(as_tibble(rbind_pages(data)))
}

# breaks the text down into single words, 
# inner joins them with the NRC library 
# and counts the frequency of each sentiment. 
join_words_with_sentiment <- function(raw_tweets) {
  nrc_sentiment <- get_sentiments("nrc") %>% 
    select(word, sentiment)
  words <- raw_tweets %>%
    unnest_tokens(word, text) %>%
    left_join(nrc_sentiment, by = "word") %>%
    mutate(sentiment = replace_na(sentiment, replace = "none")) %>%
    filter(!word %in% stop_words$word) %>%
    group_by(search, region, country, sentiment, week) %>%
    count(sentiment) %>%
    rename(words = "n")
  return(words)
}

# calculates the sentiment for each region
# as a percentage of total words. 
calculate_regional_percentages <- function(words) {
  final_value <- words %>%
    group_by(region) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, sentiment, percent) %>%
    group_by(region, sentiment) %>%
    summarise(percent = sum(percent))
  return(final_value)
}

# calculates the sentiment for each week
# as a percentage of total words. 
calculate_week_percentages <- function(words) {
  final_value <- words %>%
    group_by(region, week) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, sentiment, percent, week) %>%
    group_by(region, sentiment, week) %>%
    summarise(percent = sum(percent))
  return(final_value)
}

# calculates the sentiment for each search term
# as a percentage of total words. 
calculate_search_term_percentages <- function(words) {
  final_value <- words %>%
    group_by(region, search) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, search, sentiment, percent) %>%
    group_by(region, search, sentiment) %>%
    summarise(percent = sum(percent))
  return(final_value)
}

# calculates the sentiment for each country
# as a percentage of total words. 
calculate_country_percentages <- function(words) {
  final_value <- words %>%
    group_by(country, region) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, country, sentiment, percent) %>%
    group_by(region, country, sentiment) %>%
    summarise(percent = sum(percent))
  return(final_value)
}

# a list of different data frame names.  
dataframes <- c("11-05-2021", "18-05-2021", "25-05-2021",
                "01-06-2021", "08-06-2021", "15-06-2021", 
                "22-06-2021", "29-06-2021", "06-07-2021", 
                "13-07-2021", "20-07-2021", "27-07-2021", 
                "03-08-2021", "10-08-2021", "17-08-2021", 
                "24-08-2021", "31-08-2021", "07-09-2021", 
                "14-09-2021", "21-09-2021", "28-09-2021", 
                "05-10-2021", "12-10-2021", "19-10-2021",
                "26-10-2021", "02-11-2021", "09-11-2021",
                "16-11-2021", "23-11-2021", "30-11-2021",
                "07-12-2021", "14-12-2021", "22-12-2021")

# a combined list of raw data. 
raw_data <- read_all_data(dataframes)

# rename regions.   
raw_data[raw_data=="Sinic"] <- "Sinosphere"
raw_data[raw_data=="Anglo"] <- "Anglosphere"

# creates an index of non-duplicated tweets.
index <- which(!duplicated(raw_data[,1]))
data <- raw_data[index,]

# counts the frequency of different NRC sentiments.  
words <- join_words_with_sentiment(data) %>%
  spread(sentiment, words) %>%
  rename("Anticipation" = "anticipation", 
         "Joy" = "joy", 
         "Trust" = "trust",
         "Anger" = "anger", 
         "Disgust" = "disgust", 
         "Fear" = "fear") %>%
  gather(sentiment, words, -c(1:4)) %>%
  mutate(words = replace_na(words, replace = 0)) 

# Calculates each sentiment as a percentage of 
# total words based on region. 
regional_percentages <- calculate_regional_percentages(words)

# Calculates each sentiment as a percentage of 
# total words based on week.  
week_percentages <- calculate_week_percentages(words)

# calculates each sentiment as a percentage of 
# total words based on search term.  
search_term_percentages <- calculate_search_term_percentages(words) 

# calculates each sentiment as a percentage of 
# total words based on country.  
country_percentages <- calculate_country_percentages(words)

# my theme
my_theme <- theme_economist_white(gray_bg = FALSE) +
  theme(plot.title = element_text(hjust = 0.5, 
                                  vjust = 12, 
                                  size = 10, 
                                  color = "#474747"),
        plot.margin = unit(c(0.5, 1, 1, 1), "cm"),
        axis.text = element_text(size = 9, 
                                 color = "gray30"),
        axis.text.x=element_text(vjust = -2.5),
        axis.title.x = element_text(size = 9, 
                                    color = "gray30", 
                                    vjust = -10),
        axis.title.y = element_text(size = 9, 
                                    color = "gray30", 
                                    vjust = 10),
        legend.direction = "vertical", 
        legend.position = "right",
        legend.title = element_blank(),
        legend.text = element_text(size = 12, 
                                   color = "gray20"),
        legend.margin=margin(1, -15, 1, 0),
        legend.spacing.x = unit(0.25, "cm"),
        legend.key.size = unit(1, "cm"), 
        legend.key.height = unit(0.75, "cm"),
        strip.text = element_text(hjust = 0.5, 
                                  vjust = 1, 
                                  size = 10, 
                                  color = "#474747"),
        panel.spacing = unit(2, "lines"))

```

## **Summary**

This project uses sentiment analysis to examine 1.06 million tweets containing, or in response to those containing, one of nine anti-authority words between Anglosphere and Sinosphere Twitter users. Like James Dean’s character in Rebel Without a Cause, Anglosphere users appear to view rebellious acts more positively than others.

&nbsp;

## Results

### **1) Region:**

There is significantly lower net sentiment among Sinosphere users for the anti-authority terms overall.

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# Region
regional_percentages %>%
  spread(sentiment, percent) %>%
  mutate(net = positive - negative) %>%
  gather(sentiment, percent, -1) %>%
  filter(sentiment == "net") %>%
  ggplot(aes(x = region, y = percent, fill = region)) +
  geom_bar(stat = "identity", position = "dodge") +
  ggtitle("") +
  ylab("Net Sentiment (%)") +
  xlab("") +
  scale_fill_manual(values = c("#0072B2", "#D55E00")) +
  my_theme +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

```

&nbsp;

### **2) Search Term:**

There is also significantly lower net sentiment among Sinosphere users for each anti-authority term.

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# search terms
search_term_percentages %>%
  spread(sentiment, percent) %>%
  mutate(net = positive - negative) %>%
  gather(sentiment, percent, -c(1:2)) %>%
  filter(sentiment == "net") %>%
  ggplot(aes(x = search, y = percent, fill = region)) +
  geom_bar(stat = "identity", position = "dodge") +
  ggtitle("") +
  ylab("Net Sentiment (%)") +
  xlab("") +
  scale_fill_manual(values = c("#0072B2", "#D55E00")) +
  my_theme +
  scale_x_discrete(guide = guide_axis(n.dodge=2))

```

&nbsp;

### **3) Country:**

There is also lower net sentiment among the majority of Sinosphere countries.

```{r, echo = FALSE, message = FALSE, warning = FALSE, dpi=600}

# selected countries 
country_percentages %>%
  spread(sentiment, percent) %>%
  mutate(net = positive - negative,
         country = factor(country, 
                          levels=c("Australia", "Canada", "New Zealand", 
                                   "South Africa", "UK", "US", 
                                   "China",  "Hong Kong", "Singapore", 
                                   "South Korea", "Taiwan", "Vietnam"))) %>%
  gather(sentiment, percent, -c(1:3)) %>%
  filter(sentiment == "net") %>%
  ggplot(aes(x = country, y = percent, fill = region)) +
  geom_bar(stat = "identity", position = "dodge") +
  ggtitle("") +
  ylab("Net Sentiment (%)") +
  xlab("") +
  scale_fill_manual(values = c("#0072B2", "#D55E00")) +
  my_theme +
  scale_x_discrete(guide = guide_axis(n.dodge=2))

```

&nbsp;

## **Disclaimer**

The data above does not take into account the context of these tweets. Although the high number (over 1 million) of tweets may suggest a statistically significant relationship between higher anti-authority sentiment and Anglosphere users, a more targeted study on individual protest events would help to confirm this.

&nbsp;

## **Method**

### **1) Choose Terms:**

The terms were chosen by searching for synonyms of protest.

**Tweets (000s)**

```{r, echo = FALSE, message = FALSE, warning = FALSE}

kable(spread(data.frame(round(table(data$search)/10^3, 0)), 1, 2))

```

&nbsp;

### **2) Countries:**

The countries were selected because Anglosphere (Western) and Sinosphere (Confucian) cultures were thought to be the most different in terms of attitudes towards authority according to Hoefstede’s research on power distance.

**Tweets (000s)**

```{r, echo = FALSE, message = FALSE, warning = FALSE}

kable(spread(data.frame(round(table(data$country)/10^3, 0)), 1, 2))

```

&nbsp;

### **3) Data Collection:**

The tweets were collected from a 50 mile range of the each country capital every week from 11 May to 22 December 2021.

---EXAMPLE CODE SNIPET---

```{r, echo = TRUE, message = FALSE, warning = FALSE}

# searches for tweets with a given search term in a given 
# location using the rtweet search_tweets function. 
get_tweets <- function(search_term, coordinates) {
  search_tweets(
    q = search_term,
    n = 100000,
    include_rts = FALSE,
    retryonratelimit = TRUE,
    lang = "en",
    geocode = coordinates
  )
}

```

&nbsp;

### **4) Data Cleaning:**

The text is cleaned by removing links and converting all characters to lowercase.  

---EXAMPLE CODE SNIPET---

```{r, echo = TRUE, message = FALSE, warning = FALSE}

# cleans links from the text. 
process_raw_tweet_data <- function(country) {
  pattern <- "https://t.co/[A-Za-z\\d]+|&amp;"
  text <- country %>%
    mutate(text = str_to_lower(str_replace_all(text, pattern, "")))
  return(text)
}

```

&nbsp;
 
### **5) Sentiment Analysis:**

A sentiment analysis is calculated by breaking down each tweet into individual words, removing stopwords (common words with little sentiment value) and calculating the percentage of positive minus negative words.

---EXAMPLE CODE SNIPET---

```{r, echo = TRUE, message = FALSE, warning = FALSE}

# calculates sentiment values faceted by region, search term and sentiment type. 
calculate_search_term_percentages <- function(words) {
  final_value <- words %>%
    group_by(region, search) %>%
    mutate(total_words = sum(words),
           percent = (words / total_words)*100) %>%
    select(region, search, sentiment, percent) %>%
    group_by(region, search, sentiment) %>%
    summarise(percent = sum(percent))
  return(final_value)
}

```

&nbsp;

## **Sources**

- Clearly Cultural (2022) https://clearlycultural.com/geert-hofstede-cultural-dimensions/power-distance-index/

- Mohammad (2021) https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm

- Twitter (2021) https://developer.twitter.com/en/apply-for-access

&nbsp;
&nbsp;
&nbsp;
